[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "The most recent buzz around ML in my own observations are when I was a Statistics student in Zürich around 2019. I remember scrambling to find a job upon graduation, during a biomedical PhD that wasn’t quite the right fit for me. Needless to say, I was feeling unprepared for a world of impending uncertainties compounded by the 2020 pandemic and a moving target of what Data Scientists were for. As I’ve learned, even classical statisticians have their gaps. It’s called other-statisticians’-specialisations. Let’s digress back into ML.\nPun intended, it’s an ensemble of methods that mostly finds its basis in linear regression. Some demonstrations absorb as many features as it can find, without so much as to check for actual linear relationships, independence of covariates, colinearities or omitted variables biases, all assumptions of linear regression modeling. Others incorporate Orthogonalisation to scrupulously pick features that don’t show dependence for the convenience of model interpretation and performance (Askanazi & Grinberg, 2024). I stan for this rigour. That’s not a complete review, but one observation from a classical statistician diversifying she/her skills.\nHow I am being curious\nTo elaborate on my initial and current qualms, I struggle to find the connection between method and contribution to scientific answers with ML. I feel that is a healthy struggle in which fuels my curiosity. I come from health care as well, and with this duality, I feel an accountability as the analyst, to explain the connection between the choice of model, parameters, to its contribution to the scientific question. Neural Networks are transformative to computer vision questions, and while I dabbled with it with the Julia Language, I am limited to explain why my choice of number of layers, type of layers and learning rate, for example, should be used other than the fact that it gives for nice diagnostic results such as accuracy, precision, positive predictive probability, Sensitivity or False positive rate.\nBut what about the possibilities that Computer Vision can predict and prevent full blown cancer metastases ? As an Oncology RN myself, I would support the complete shut down of these pre-cursory processes. The pain and suffering, and absorption of resources of Cancer in the developed world far outweigh actual solutions we have for them. We have urgencies to explore further.\nIn the Commercial and Healthcare realm, if that dichotomy exists for you, I see the benefits of the favourable options given by an ML model’s ROC that can potentially cover all budget scenarios. ML models that are good, affect money and the allocation of health care resources. It can’t get more IRL (in real life) than that. I accept assumptions, I acknowledge the arbitrariness of parameter choices affects my confidence for reproducibility and therefore inference for RL things. I question : Can a trained model on data, which will inevitably be older data than the predictions we need today, be reproducible ? As we observe in the news, the past seems to repeat itself. Scary as it sounds, but trained models have also done so much good and bad already, so why not take the positive forces and go with them ?\nFeature selection, and the important features\nI’ve learned over time the elegance of these words, while summary statistics from classical regression can already point to what features seem to contribute the variation of the outcome’s mean, and which to the most extent (important feature), and with what uncertainty (variance or standard deviation). Learning ML truly lends itself to Data Scientists becoming more acute in elegant commercial terms.\nIn the same commercial world, I’ve learned that ML models ingest tens of thousands of features, without any evidence of sense checking if they meet linear regression assumptions. Furthermore, classical model evaluation techniques such as the AIC (Akaike’s information criterion) and BIC (Bayesian information criterion) don’t have main character energy in the ML literature I’ve come across in the last five years. Then again, five years is not a lot, and there are more semantically sound diagnostics such as the ones mentioned before. Here are the respective formulas of the afore mentioned classical techniques. L hat being the likelihood, k being the number of parameters, n being the number of observation points.\nOne can see that the choice of which model diagnostic to use is based on where the Data Scientist wants the model to be most penalised for ; number of observations or number of parameters. I posit that Data Science in Big Tech would tend to BIC owing to the number of rows their data may have, life sciences would tend to AIC, especially in experimental settings where the number of variables are even fewer, if used at all. That is also a broad stroke on this topic of which I have no doubt have finer layers to its story.\nBack to the topic of feature selection. Linear regression assumptions cover the systematic approach to ensuring we minimise multicolinearity (the numeric teaming up of independant variables), autocorrelation (errors between data points trend together) and omitted variable biases (we didn’t forget to include an important feature that show up as inflated error). Reservations against ML can rather come from a place of concern for model integrity and interpretability, and personally for me, the accountability we have on the decisions we make because of it. I hope that those who need “Data Scientists” can be fidel to this scrupulous process that has benefited the good decisions we have made already based on good practice.\nThe importance of code engineering\nI insist that writing code is tied even to the development and learning for classical statisticians. Writing code is a doorway to furthering more technical knowledge just by the sheer efficiency of analysis made by using software alone. If there are gaps to our technical knowledge, writing code makes us fail faster. I have observed this myself as a lead developer of the R package phase1b. I would argue, that being a hands on analyst, is THE duality that is a Statistician or Data Scientist. Thus I propose that one of the barriers of ML is the plethora of software availability just in one language. R, Python and Julia have their own families of ML softwares, and how we may learn of them seem to be at the mercy of such things as the word of mouth, or the power of our prompt engineering (search and or AI tools) just to name a few. The downloads of ML related R packages make some currently the around top 500 most downloaded R packages out of over 20’000 packages on CRAN. See “yardstick” and “tidymodels”. It is surely confounded by the buzz, but ML skills are a ubiquitous part of the market forces and learning about multitude of softwares to that end is increasingly a barrier.\nIf classical statisticians want to finally find comfort in ML tools, more can be done to clarify their qualms. One such of mine is connecting the choice of Neural Network parameters to the scientific question. I marvel at the results yet cannot come to terms with the how. Furthermore, the search for improving the reliability of prediction error is another such qualm. Overall, coding skills are a pathway to understand what they are and an attempt at solutions rather than stagnating at problems.\nFor real though, why classical statisticians aren’t wrong to be curious\nThe dichotomy of classical Statistics and ML comes across as more pronounced to classical Statisticians. This may be the basis for our curiosity. The earliest documentation I have found is 1959 of when the term was coined and tied to such things as Artificial Intelligence and Computer Vision. As per the Wikipedia article :\nMachine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions\nAn observation is that the single distinction that would tend a classical statistician toward ML is an emphasis on prediction. Yet, there is no shame in wanting to predict. Off-hand comments among classical statisticians, for the lack of better word, boycott ML terminologies. But this is limiting is so many ways. I get the things the Montagues are saying about the Capulets but I respectfully question persistent trauma bonds. Any RL questions can be asked in a scientific framework. Many business questions are translated in Data Science terms, or Statistical terms, and then solved with a bunch of caveats. While the off-hand comments are sometimes reasonable, the truth is, there are real contributions of Machine Learning. The invisible hand of the market is pointing us to ML. And it is not necessarily wrong to do so.\nClassical Statisticians towards pragmatism\nLet’s first address our barriers : choice of ML methodology, the moving parts of parameters options such as learning rate, the multitude of ways of cross validation, the overwhelming number of ways to handle features, substantiating the rationale for those chosen paths, are merely new learnings. If these “extra stuff” has been useful in the past, shouldn’t we include and improve on them ? It seems, there is room for tools to be sharper with the advent of technology creating ways for efficiency gains (DeepSeek just entered the chat). Furthermore, learning ML for classical statisticians is already a great achievement with the evolving knowledge and softwares that make these efforts, kind of prohibitive. Let’s be real about that. Finally, I posit that learning Machine Learning if that is so a choice of a classical statistician, should be approached with a united focus on solving the actuals problems we have.\nI write this substack to share my journey in hope that it inspires others to expand their perceptions. For sure, some points can be improved and be called broad strokes, to that I encourage you to paint the finer lines in your politest form."
  },
  {
    "objectID": "blog.html#classical-statisticians-arent-wrong-to-be-curious-about-learning-machine-learning-ml",
    "href": "blog.html#classical-statisticians-arent-wrong-to-be-curious-about-learning-machine-learning-ml",
    "title": "Blog",
    "section": "",
    "text": "The most recent buzz around ML in my own observations are when I was a Statistics student in Zürich around 2019. I remember scrambling to find a job upon graduation, during a biomedical PhD that wasn’t quite the right fit for me. Needless to say, I was feeling unprepared for a world of impending uncertainties compounded by the 2020 pandemic and a moving target of what Data Scientists were for. As I’ve learned, even classical statisticians have their gaps. It’s called other-statisticians’-specialisations. Let’s digress back into ML.\nPun intended, it’s an ensemble of methods that mostly finds its basis in linear regression. Some demonstrations absorb as many features as it can find, without so much as to check for actual linear relationships, independence of covariates, colinearities or omitted variables biases, all assumptions of linear regression modeling. Others incorporate Orthogonalisation to scrupulously pick features that don’t show dependence for the convenience of model interpretation and performance (Askanazi & Grinberg, 2024). I stan for this rigour. That’s not a complete review, but one observation from a classical statistician diversifying she/her skills.\nHow I am being curious\nTo elaborate on my initial and current qualms, I struggle to find the connection between method and contribution to scientific answers with ML. I feel that is a healthy struggle in which fuels my curiosity. I come from health care as well, and with this duality, I feel an accountability as the analyst, to explain the connection between the choice of model, parameters, to its contribution to the scientific question. Neural Networks are transformative to computer vision questions, and while I dabbled with it with the Julia Language, I am limited to explain why my choice of number of layers, type of layers and learning rate, for example, should be used other than the fact that it gives for nice diagnostic results such as accuracy, precision, positive predictive probability, Sensitivity or False positive rate.\nBut what about the possibilities that Computer Vision can predict and prevent full blown cancer metastases ? As an Oncology RN myself, I would support the complete shut down of these pre-cursory processes. The pain and suffering, and absorption of resources of Cancer in the developed world far outweigh actual solutions we have for them. We have urgencies to explore further.\nIn the Commercial and Healthcare realm, if that dichotomy exists for you, I see the benefits of the favourable options given by an ML model’s ROC that can potentially cover all budget scenarios. ML models that are good, affect money and the allocation of health care resources. It can’t get more IRL (in real life) than that. I accept assumptions, I acknowledge the arbitrariness of parameter choices affects my confidence for reproducibility and therefore inference for RL things. I question : Can a trained model on data, which will inevitably be older data than the predictions we need today, be reproducible ? As we observe in the news, the past seems to repeat itself. Scary as it sounds, but trained models have also done so much good and bad already, so why not take the positive forces and go with them ?\nFeature selection, and the important features\nI’ve learned over time the elegance of these words, while summary statistics from classical regression can already point to what features seem to contribute the variation of the outcome’s mean, and which to the most extent (important feature), and with what uncertainty (variance or standard deviation). Learning ML truly lends itself to Data Scientists becoming more acute in elegant commercial terms.\nIn the same commercial world, I’ve learned that ML models ingest tens of thousands of features, without any evidence of sense checking if they meet linear regression assumptions. Furthermore, classical model evaluation techniques such as the AIC (Akaike’s information criterion) and BIC (Bayesian information criterion) don’t have main character energy in the ML literature I’ve come across in the last five years. Then again, five years is not a lot, and there are more semantically sound diagnostics such as the ones mentioned before. Here are the respective formulas of the afore mentioned classical techniques. L hat being the likelihood, k being the number of parameters, n being the number of observation points.\nOne can see that the choice of which model diagnostic to use is based on where the Data Scientist wants the model to be most penalised for ; number of observations or number of parameters. I posit that Data Science in Big Tech would tend to BIC owing to the number of rows their data may have, life sciences would tend to AIC, especially in experimental settings where the number of variables are even fewer, if used at all. That is also a broad stroke on this topic of which I have no doubt have finer layers to its story.\nBack to the topic of feature selection. Linear regression assumptions cover the systematic approach to ensuring we minimise multicolinearity (the numeric teaming up of independant variables), autocorrelation (errors between data points trend together) and omitted variable biases (we didn’t forget to include an important feature that show up as inflated error). Reservations against ML can rather come from a place of concern for model integrity and interpretability, and personally for me, the accountability we have on the decisions we make because of it. I hope that those who need “Data Scientists” can be fidel to this scrupulous process that has benefited the good decisions we have made already based on good practice.\nThe importance of code engineering\nI insist that writing code is tied even to the development and learning for classical statisticians. Writing code is a doorway to furthering more technical knowledge just by the sheer efficiency of analysis made by using software alone. If there are gaps to our technical knowledge, writing code makes us fail faster. I have observed this myself as a lead developer of the R package phase1b. I would argue, that being a hands on analyst, is THE duality that is a Statistician or Data Scientist. Thus I propose that one of the barriers of ML is the plethora of software availability just in one language. R, Python and Julia have their own families of ML softwares, and how we may learn of them seem to be at the mercy of such things as the word of mouth, or the power of our prompt engineering (search and or AI tools) just to name a few. The downloads of ML related R packages make some currently the around top 500 most downloaded R packages out of over 20’000 packages on CRAN. See “yardstick” and “tidymodels”. It is surely confounded by the buzz, but ML skills are a ubiquitous part of the market forces and learning about multitude of softwares to that end is increasingly a barrier.\nIf classical statisticians want to finally find comfort in ML tools, more can be done to clarify their qualms. One such of mine is connecting the choice of Neural Network parameters to the scientific question. I marvel at the results yet cannot come to terms with the how. Furthermore, the search for improving the reliability of prediction error is another such qualm. Overall, coding skills are a pathway to understand what they are and an attempt at solutions rather than stagnating at problems.\nFor real though, why classical statisticians aren’t wrong to be curious\nThe dichotomy of classical Statistics and ML comes across as more pronounced to classical Statisticians. This may be the basis for our curiosity. The earliest documentation I have found is 1959 of when the term was coined and tied to such things as Artificial Intelligence and Computer Vision. As per the Wikipedia article :\nMachine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions\nAn observation is that the single distinction that would tend a classical statistician toward ML is an emphasis on prediction. Yet, there is no shame in wanting to predict. Off-hand comments among classical statisticians, for the lack of better word, boycott ML terminologies. But this is limiting is so many ways. I get the things the Montagues are saying about the Capulets but I respectfully question persistent trauma bonds. Any RL questions can be asked in a scientific framework. Many business questions are translated in Data Science terms, or Statistical terms, and then solved with a bunch of caveats. While the off-hand comments are sometimes reasonable, the truth is, there are real contributions of Machine Learning. The invisible hand of the market is pointing us to ML. And it is not necessarily wrong to do so.\nClassical Statisticians towards pragmatism\nLet’s first address our barriers : choice of ML methodology, the moving parts of parameters options such as learning rate, the multitude of ways of cross validation, the overwhelming number of ways to handle features, substantiating the rationale for those chosen paths, are merely new learnings. If these “extra stuff” has been useful in the past, shouldn’t we include and improve on them ? It seems, there is room for tools to be sharper with the advent of technology creating ways for efficiency gains (DeepSeek just entered the chat). Furthermore, learning ML for classical statisticians is already a great achievement with the evolving knowledge and softwares that make these efforts, kind of prohibitive. Let’s be real about that. Finally, I posit that learning Machine Learning if that is so a choice of a classical statistician, should be approached with a united focus on solving the actuals problems we have.\nI write this substack to share my journey in hope that it inspires others to expand their perceptions. For sure, some points can be improved and be called broad strokes, to that I encourage you to paint the finer lines in your politest form."
  },
  {
    "objectID": "blog.html#an-approach-to-making-hypothesis-testing-more-approachable",
    "href": "blog.html#an-approach-to-making-hypothesis-testing-more-approachable",
    "title": "Blog",
    "section": "An approach to making Hypothesis Testing more approachable",
    "text": "An approach to making Hypothesis Testing more approachable\n\nType II error here, unlike Statisticians, is the real Party Pooper\nThe Motivation to be Nicer\nI get a sense that a lot of the intimidation of Hypothesis Testing and Statistics point to the missing information of the “philosophy” behind it. “Paradigm” if you so will. More specifically, on how much more there is to know before we can fully grasp the procedures of Hypothesis Testing. When I was working at the University of Zürich’s Centre for Reproducible Science, I got the sense also that issues in Science that can be solved when this knowledge is delivered in an approachable way. I saw the opportunities to attentively listen and respond to the qualms of those desperately wanting to be part of a Scientific discovery, yet limited in their pursuit. Twitter at that time, was a marketplace of broad strokes that missed the multifaceted nature of a cry for help, on both sides. Even if Twitter has evolved to a distasteful marketplace of remarks, we can still advance Science by paving learning paths towards properly grasping Hypothesis Testing. I will attempt to embody the following words in my approach :\nIt is nice to be important, but it’s important to be nice, JMT\nThe Gist of our barriers\nThe science of statistics isn’t about taking the stance on any hypothesis, however biological male homo sapiens still cannot get pregnant.\nI acknowledge the efforts of a field like Cognitive Science to highlight the traps our minds fall into when thinking about Hypothesis Testing. The latter is a tool to approach a testable statement that bears percentages, with many “hold on to that thought” preceding moments that I would like to address. For those who don’t know,\nCognitive Science views human cognition as information processing and provides an inter-disciplinary integration of approaches from cognitive psychology, informatics (e.g., artificial intelligence), neuroscience and anthropology among others, CH from ETH\nI am not a Cognitive Scientist, but I hope it highlights that thinking about thinking is more than a thing, it is a field. In my attempt to explain Hypothesis Testing, I wholeheartedly want to acknowledge the mental gymnastics involved in this crucial tool in Science. That nothing of this is “basic” and slow and steady arrivals at understanding is a great achievement that should never be shamed. Our brains are influenced by the bias that our testable hypotheses come from. They are also tainted by the misunderstandings of “significance”. In addition, p-values thresholds are made-up and yet nothing is proven except the ability to run an experiment. Holding promised assumptions in numbers and complex concepts all at the same time is brain training, but I would argue, a worthy one. The science of statistics is isn’t about taking on any hypothesis, however biological male homo sapiens still cannot get pregnant.\nLet’s only talk about Experiments from now on\nA few starting assumptions\nThe assumption is that every phenomenon in this universe has its distribution\nWhen designing an experiment, there is a framework that can qualify with numbers, how likely a phenomenon is true. That framework is Hypothesis testing. Hold that thought.\nThe assumption is that every phenomenon in this universe has its distribution, parametric or not (come from a fanciful bell curve or not). Through established methods (“methods of moments” as one example), we can pull out useful estimates such as mean, median, mode, variance and so on and so forth to describe THAT phenomenon. Now hold this thought.\nWhat is a distribution ? A Y-axis and an X-axis represent some values. The Y being spanning from 0 to 1, or 0% to 100%, and expression of how likely, or frequently, any value of X is occurring at. Hang in there, one more concept to go : The Law of Large numbers tells us that if this phenomenon was repeated many times, say 100, a neat distribution is revealed pointing to a bell curve that forms the foundation of hypothesis testing. The curve or distribution is usually symmetric, with equal sized tails. The center is where the mean meets the median, due to this symmetry.\nThe experiment is like to be only ONE experiment, but the Hypothesis Testing is a framework for a simulation that likens to the Law of Large Numbers, just that our numbers may be smaller and a reflection of real-life limitations. Funding and ethical qualms will be at hand at the attempt to repeat experiments up to 100’000 times just to smooth out histograms that liken to nice bell curves. This is why sampling methods are so popular. We will not discuss them in this substack, yet. However, we can run an experiment though, through software. “Simulation” is a term that comes up again and again as a thought experiment if you like, of what would happen if we had MANY experiments. This leads me to discuss confidence intervals because what does confidence mean ? For the Frequentist framework (the “default” one), confidence is given to an estimate in the setting of the Hypothesis Test, merely because we assume a phenomenon occurs under a repeatable distributions (plural). There are two interpretations of this interval of confidence :\nFrequentist /Classical/ “default” interpretation : This is what most non-Statisticians have heard about and the focus on this substack. It is tied to the Law of Large numbers where if we ran the experiment 100 times, a 95-percent-confidence-interval is saying that 95 of 100 of those confidence intervals contain the true value. That is a mildly generous amount, because of 2 :\nBayesian calls them “credible intervals” : The popularity of the Bayesian approach is the feature that the “confidence interval” contains the true value - this has a more salient interpretation! The credible interval accounts for the uncertainty and bounds the true value. This distribution is not repeated which zeroes in on the specificity of the phenomenon e.g. its population inferenced. All of this is more precise than what 1 has to say.\nStatistical Models should remain as such\nGeorge Box had a distasteful and celebrated saying that “Statisticians, like artists, have the bad habit of falling in love with their models.” I vehemently oppose the implicit message of misogyny in this, and urge you as well to be part of the solution. So stay focussed and ask that others do so as well.\nDigressing back to the assumption that every phenomenon has a distribution : This is where the term “model” comes in. A model approximates important estimates to understand the phenomenon. A model which can be made up of limited data, can limit the inference or generalisation to larger populations that did not make that model. Bayesian methods attempt to approximate a model to be as likely as possible to the phenomenon with priors being assembled with observed data (through the likelihood function) to create neat new “posteriors”, terms chosen to reflect the workflow. These posteriors can express mean, median, mode (even if counterintuitive) and other useful characteristics of the phenomenon it is representing. Remember, that either a Frequentist or Bayesian approach, a representation has its degrees of uncertainty when it tries to mirror a real phenomenon. Everyone, including the Statistician, is at the mercy of its approximation.\nFor more information about Artists in the time of George Box, checkout Pablo-matic for more, or his granddaughter’s words.\nOur first simulation and the Law of Large Numbers\nWe know that a phenomenon can be modelled. The model can still be wrong. But let’s try creating a distribution while dictating its mean and variance. I am confident a lay person can get the gist of the code below, and in the spirit of transparency, also want to include it even if it will be skipped.\n\n\n\n\n\n\n\n\n\n\n\n\nThe following is data forming a distribution. There are 100 separate data points, the most frequent at nearly 60 %, the least at 0 %. Most of the data lie in the middle. Indeed there is a skew to your left. This means that less data are found between 1-3 on the X axis. One can argue that there is also a right skew with data between 6-7 occurring very infrequently. I hope one can appreciate that this is an experiment.\nI attempt to emulate the Law of Large Numbers now, by increasing the number of X axis random variables we can sample. We sampled 100 data points before, and now we will do 1000, a ten fold increase :\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow imagine if we took estimates from the green, red and purple graphs. The mean is probable the same, but the variance improves (decreases) eventually. That is precisely why small sample sizes trigger Statisticians against allowing persuasive conclusions on the estimate. But the Law of Large numbers is on our side on two conditions: if our data grows, and if it does so in quality.\nThe Null Hypothesis\nThe phenomenon in question here is “Nothing happens”. The following are examples of Alternative Hypothesis, nullified, to bring the point home about how to construct a Null Hypothesis.\n1 pint of Beer gives you 20mg of Vitamin B12 → 1 pint of Beer has zero Vitamin B12\nThe novel drug cures cancer → The novel drug did nothing\nMy cat is a dog → My cat is nothing\nNotice that these statements are not binary opposites. But binary thinking does exist in the Hypothesis Testing workflow. Which is where we fall into cognitive traps of convenient thinking. Hold that thought. Binary thinking does not exist here:\nIf the Null Hypothesis is rejected, it does not mean the Alternative Hypothesis is true. It just means the Null Hypothesis is rejected. The conclusion be, that there is a belief that the Null Hypothesis is not true, and there is probable cause to believe that the Alternative Hypothesis is true. But the Alternative Hypothesis at this stage is not necessarily true.\nThe point of emphasis here, is that Hypothesis Testing will either not reject or reject the Null Hypothesis even if anything can be true. With a large and well designed experiment however, we can shift greater confidence to either the phenomenon of the Null or Alternative Hypotheses’ distribution. If the Hypothesis Testing results in the “wrong” Hypothesis to be favoured, it could be a problem of the design itself (poor sample representation of the population). Conclusion : Results of Hypothesis Testing can favour the truth about the universe, or not, and has nothing to do with it. It’s just a statistical test.\nWe can be wrong about the truth of the universe, whether 1 pint of beer has Vitamin B12 that is in excess of 0, and we can also be wrong about favouring the wrong hypothesis. Let’s have a look at how. From now on, we include thinking in Binary Opposites : type I error and its complement, verses type II error and its complement.\nThe probability that we’re wrong about the Null Hypothesis\nObserve the purple graph, and the nicely shaped skewed tails on either side. The purple graph talks about a phenomenon, that 1 pint of Beer gives you zero Vitamin B 12, that is a Null Hypothesis. However, after the experiment, the results show that there seems to be another whole truth about it, another whole distribution.\nAlways write your Null and Alternative Hypothesis out. Yes, it’s a very focussed Scientific question.\n                    H0 = 1 pint of Beer gives 0 mg of Vitamin B12\n\n                   HA = 1 pint of Beer gives &gt; 0 mg of Vitamin B12\nWe are not yet done with a designed experiment yet. What is a good enough effect size (the minimal detectable difference that Vitamin B12 gives) to show that it is a significant amount of Vitamin B12 ? Let’s say, it’s not zero. But by what margin ? Hold that thought.\nLet’s also remember that this is a one-sided test. Which means we are testing AGAINST the “Nothing happens” hypothesis on whether the evidence is going into one direction.\nAfter constructing a solid Null Hypothesis, we can be wrong about the Null Hypothesis. That’s so Alpha.\nAlpha is the allowance we give ourselves in the realm of the Null Hypothesis that we could be wrong, about this realm. When we are wrong, that is synonymously known as an Error. More specifically if we are wrong in this case, it is called a Type I error.\nWe set a threshold in percentage, on what proportion of result we would discard the Null Hypothesis altogether. The evidence may even be pointing to the Alternative Hypothesis if the Vitamin B12 level comes up as higher than zero (not lower because we have defined the Alternative hypothesis to be higher).\nAlpha is this precise proportion. If we set alpha = 5%, that means 5 of 100 results that are in the realm of the Null Hypothesis (or its distribution) are non-believable. Hold that thought. Now if say we do get a result that is in this corner of the distribution, that 5%, and we reject the Null Hypothesis, we are following the Hypothesis testing process. Remember when I said that the Null and Alternative Hypotheses aren’t binary opposites ? Remember when I said “Hypothesis Testing will either not reject or reject the Null Hypothesis even if anything can be true.” These seem very much contrary but important to the premises to Alpha. Let me explain.\nI want to introduce the Type I error here, that is officially defined as:\nThe probability of rejecting the Null Hypothesis when it is actually true\nAlpha is the allowance we give ourselves in the realm of the Null Hypothesis that we could be wrong, about this realm. When we are wrong, that is synonymously known as an Error. More specifically if we are wrong in this case, it is called a Type I error.\nWhen we have committed a Type I error, it still doesn’t mean the Null Hypothesis is THE real universe, but assuming when we assume it is, then we have committed a Type I error. (Alpha is the defined realm where we allow our Null Hypothesis to be an erroneous representation of a phenomenon because those 5% of values represent such extremes that stray from what’s more likely in the Null Hypothesis’ realm).\nAnd Yes, evidence pointing to a male human being pregnant is a Type I error.\nType II Error is the real party pooper\nType II error : Those of the world with a Negative Bias can feast on this.\nThe adage of willing negative events by thinking they will occur, is appropriate here. Let’s suppose that wanting the nutritional value of Vitamin B12 to be substantial in 1 pint of beer is socially encouraged. The party poopers want to see the opposite and they will will it. We have room for the party poopers too, it’s called Beta. There is another player in the construction of the Alternative hypothesis’ distribution, and that is power and effect size. Power is 100% minus Beta, so they are opposites. To have high power is to have a low margin of Type II Error. Power is easier to interpret as it is the proportion of True Positives. As for effect size, let’s just say for now, that it has been calculated. Back to Beta, just as the previous example, we allow ourselves to be wrong about the Alternative realm by a margin of Beta, or let’s say 10%. That is, if the Alternative realm is true, we allow a10 % region of Beta as a magnitude of our poor judgement. If our evidence indeeds falls into this Beta region, and we preserve the Null Hypothesis (not reject the Null), AND we are wrong to do so because the Alternative realm is true, then we have committed a Type II Error.\nType II error is defined as :\nNot rejecting the Null Hypothesis when it is not true. Remember this does not necessarily mean the truth is the Alternative Hypothesis, but when we assume it is, then we have committed a Type II Error. Similarly we can say that the direction of the evidence within this Hypothesis Test is giving more confidence to the Alternative Hypothesis being true.\nIf we go through life with great suspicion of good will and generosity and perceive ulterior and evil as motives to them, and we are wrong to do so, it can also be labeled as a Type II error, assuming the good will was what was happening. Our negative judgement was a false negative. We were wrong to think that behind every good will were bullet points of agendas. Similarly, appraising someone’s performance as bad is an analogue as to when we are only seeking evidence to that direction. Those of the world with a Negative Bias can feast on this, and apparently, not much more.\nPictorial summaries are just as important as being Nice\nThe following image mimics the matrix comparing what is truth and what we do to the Null hypothesis. Type I error is when a man is told they are pregnant, and Type II error is when a pregnant woman is told she isn’t pregnant. This has been one milestone of statistical communication I stan. I can see my arguments thus far can be flawed by the nuance in what realm is actually the true realm. Remember, it is only when we assume one realm is true, Null or Alternative, then it is possible to make an error of judgement.\nSo you want to be negative, then negative shall come to you : Futility interim analyses.\nAs just alluded, everytime we come into an exploration with a hammer, we will find nails or appropriate non-nails into one. The futility interim analyses is much like this, where in an experiment such as a drug trial, at every “look” we take on data, we increase the possibility of meeting the futility criteria (finding nails), just because we are looking for it ! With each “look”, we increase our chances for a false negative. Assessing for futility in the interim AND final assessment of data means we have more possibilities to find false negative, there by accumulating our Type II error. Thus it is imperative to control for Type II error when accessing the collection of data more than once during a drug trial. There are mathematical tools for this, but I wanted to extend the point of Type II errors to the topic of Futility Interim Analysis. Furthermore, as we know the complementary relationship of Beta and Power, when we pile on the Beta, we eat out the Power, leaving less power to accompany the final result.\nIt’s still about being Nice\nKindness is not weakness. Arriving in learning through kindness is one such strength.\nA lay person may comment that the Hypothesis testing workflow is a very accountable way to assess our Scientific question. That would be a sound conclusion. Intuitions on where the truth is can still exist in the opposite direction of what a Hypothesis test concludes in, but Science is a deductive process and benefits from focussed and systematic approaches. There are many mind traps in this complex topic and I suggest credible resources that speak best to you. This is only one attempt at being part of the solution. I still wholly encourage you to make many iterations of learning this complex theory, precisely because it is complex, even for Statisticians. As for Statisticians, I urge you to find within yourself the compassion and patience once afforded to you when learning of these complex notions - even if it was only you who were providing it. After all, kindness is not weakness. Arriving in learning this through kindness is one such strength. With what Science has achieved so far, and what scientists need in order arrive robustly at new knowledge or a certainty of the lack thereof, I hope learning about Hypothesis Testing makes for everyone, a worthwhile investment.\nI would like to acknowledge, people I would call friends, who have given me safe spaces, to iterate and navigate in my own journey of learning.\nReferences (my fav resources in general)\nStatistical Power by PSU https://online.stat.psu.edu/stat415/lesson/25/25.1\nStatistical Power by statshowto https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/statistical-power/"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Research Software",
    "section": "",
    "text": "coming soon…"
  },
  {
    "objectID": "dscience.html",
    "href": "dscience.html",
    "title": "Data Science",
    "section": "",
    "text": "coming soon…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Finc about us",
    "section": "",
    "text": "Finc Research Company is a remote firm specialising in :\n\nData Science consulting\nResearch Software consulting\nSpeech, text ghost writing\n\nOur services are inclusive to many sectors including small to large scale biotechs, agriculture, non-for-profits, robotics and healthcare."
  },
  {
    "objectID": "Software.html",
    "href": "Software.html",
    "title": "Software",
    "section": "",
    "text": "Coming soon."
  }
]